{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Data management with Pandas\n",
    "Pandas is used to manipulate, clean, and query data by looking at the Pandass data tool kit. Pandass was created\n",
    "by Wes McKinny in 2008 and is an open source project under a very permissive license.\n",
    "\n",
    "## Exercise 1: Pandss Series data structure\n",
    "If you are familiar with Pandas and Series manipulation, you can skip this one and go to the second exercise. To\n",
    "start, you will create a new notebook using your local environment or colab environment. You have to name your\n",
    "notebook with PW2-[firstName Lastname MajorName]. Now you can answer the following questions\n",
    "1. import Pandas and create a list of sports containing this list [\n",
    "′Football′\n",
    ",\n",
    "′ HandBall′\n",
    ",\n",
    "′ SnowSport′\n",
    "] and\n",
    "store it in a variable sports;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a list of sports\n",
    "sports = ['Football', 'HandBall', 'SnowSport']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. cast your list as a series and display sports using the Class Pandass.Series (see documention on class Pandass.Series(data=None, index=None, dtype=None, name=None, copy=None, fastpath=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Football\n",
      "1     HandBall\n",
      "2    SnowSport\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "sports_series = pd.Series(sports)\n",
    "print(sports_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. what is the type of each element of your list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "for item in sports:\n",
    "    print(type(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create a new list named numeric containing a short list of numbers and display it as a series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     5\n",
      "1    10\n",
      "2    15\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "numeric = [5, 10, 15]\n",
    "numeric_series = pd.Series(numeric)\n",
    "print(numeric_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. add the value None to both animals and numeric and compare their corresponding display with Series.\n",
    "What did you remark?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      NaN\n",
      "1     15.0\n",
      "2    100.0\n",
      "dtype: float64\n",
      "0    BasketBall\n",
      "1      HandBall\n",
      "2          None\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "numeric_with_none = [None, 15, 100]\n",
    "sports_with_none = ['BasketBall', 'HandBall', None]\n",
    "\n",
    "numeric_series_with_none = pd.Series(numeric_with_none)\n",
    "sports_series_with_none = pd.Series(sports_with_none)\n",
    "\n",
    "print(numeric_series_with_none)\n",
    "print(sports_series_with_none)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "\n",
    "When None is added to the numeric list, it is converted to NaN in the series.\n",
    "In the sports list, None appears as an empty object in the series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. We shall correct the incomplete value None of sports by changing the None by NaN (Numpy.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    BasketBall\n",
      "1      HandBall\n",
      "2           NaN\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sports_with_nan = ['BasketBall', 'HandBall', np.nan]\n",
    "sports_series_with_nan = pd.Series(sports_with_nan)\n",
    "print(sports_series_with_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Let construct our variable sports from a dictionary as follows: {'BasketBall', 'HandBall', 'Snowsport',\n",
    "'baseBall', 'Swimming'}. Cast the variable to a serie and store it a new variable sIndex. Display sIndex\n",
    "and call the function index on it. What did you remark?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasketBall    HandBall\n",
      "Snowsport     baseBall\n",
      "Swimming          None\n",
      "dtype: object\n",
      "Index(['BasketBall', 'Snowsport', 'Swimming'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "sports_dict = {'BasketBall': 'HandBall', 'Snowsport': 'baseBall', 'Swimming': None}\n",
    "sIndex = pd.Series(sports_dict)\n",
    "\n",
    "print(sIndex)\n",
    "print(sIndex.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Series Querying\n",
    "Let we consider the variable sports from a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bask    BasketBall\n",
       "hand      HandBall\n",
       "snow     Snowsport\n",
       "base      baseBall\n",
       "swim      Swimming\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.isnan(np.nan)\n",
    "sports={'bask': 'BasketBall', 'hand': 'HandBall', 'snow': 'Snowsport', 'base': 'baseBall','swim': 'Swimming'}\n",
    "#cast the list on a serie\n",
    "sIndex=pd.Series(sports)\n",
    "#display the serie\n",
    "display(sIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. print the second element of the variable sports and find the element who has ’swim’. (use iloc[] to index\n",
    "location and loc[] to label location).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HandBall\n",
      "Swimming\n"
     ]
    }
   ],
   "source": [
    "# Print second element using iloc (index-based)\n",
    "print(sIndex.iloc[1])  # Second element based on position\n",
    "\n",
    "# Find the element labeled 'swim' using loc (label-based)\n",
    "print(sIndex.loc['swim'])  # Element with label 'swim'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Transform all items of your list sports in an uppercase characters (see Series.str.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bask    BASKETBALL\n",
      "hand      HANDBALL\n",
      "snow     SNOWSPORT\n",
      "base      BASEBALL\n",
      "swim      SWIMMING\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "sIndex_upper = sIndex.str.upper()\n",
    "print(sIndex_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. write a simple loop to transform sports in an uppercase Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    BASKETBALL\n",
      "1      HANDBALL\n",
      "2     SNOWSPORT\n",
      "3      BASEBALL\n",
      "4      SWIMMING\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "sIndex_upper_loop = pd.Series([item.upper() if item else None for item in sIndex])\n",
    "print(sIndex_upper_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. compare the runtime between both solutions and explain the runtime gap (see %timeit which is an IPython\n",
    "magic function, which can be used to time a particular piece of code.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.9 µs ± 1.15 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "55 µs ± 1.11 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Using Series.str.upper() with vectorization\n",
    "%timeit sIndex.str.upper()\n",
    "\n",
    "# Using a loop\n",
    "%timeit pd.Series([item.upper() if item else None for item in sIndex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "The Series.str.upper() function is vectorized and runs faster because Pandas internally optimizes operations on entire arrays.\n",
    "The loop, though simple, operates element by element, making it slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. compare the runtime between the function np.mean() and a loop to calculate the mean value of your numeric\n",
    "variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.9 µs ± 1.44 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "numeric = [None, 15, 100]\n",
    "num = pd.Series(numeric)\n",
    "%timeit np.mean(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.5\n"
     ]
    }
   ],
   "source": [
    "mean_value = 0\n",
    "total = 0\n",
    "count = 0\n",
    "for item in num:\n",
    "    if pd.notna(item):  # Skip NaN values\n",
    "        total += item\n",
    "        count += 1\n",
    "mean_value = total / count if count != 0 else 0\n",
    "print(mean_value)\n",
    "\n",
    "%timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of Runtime Difference:\n",
    "\n",
    "Vectorization (using np.mean()): This approach is faster since NumPy operations are highly optimized for performance and can handle entire arrays at once.\n",
    "Loop: It involves explicit iteration over elements, which is slower, especially for large datasets, because each operation is executed individually rather than leveraging efficient underlying array operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: data Verification\n",
    "## Exercise 3: From CSV to DataFrame\n",
    "As a first step, we need python libraries allowing us to load our data as a dataframe. You have to download the\n",
    "csv file named Custemers.csv. This file contains 15 columns separated with a ’,’ character.\n",
    "\n",
    "1. Load the file Custemers.csv and store its content into a dataframe variable Custemers Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file into a DataFrame (ensure the path to the file is correct)\n",
    "Custemers_Data = pd.read_csv('data/Custemers.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Display the shape of your dataframe and display the header to understand the meaning of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 15)\n",
      "                                                name  \\\n",
      "0                                       Deonte Stark   \n",
      "1                                     Faustino Boyer   \n",
      "2  Eddy Bogisich,33431 Dollie Squares Apt. 654,Po...   \n",
      "3                                     Mervyn Kreiger   \n",
      "4  Katlyn Doyle,4650 Beer Crossing Suite 848,Nort...   \n",
      "\n",
      "                        address             city           state         zip  \\\n",
      "0            278 Mueller Plains       North Euna         Alabama  03404-4384   \n",
      "1  70244 Skiles Falls Suite 030  North Altohaven      California  01522-1310   \n",
      "2                           NaN              NaN             NaN         NaN   \n",
      "3            376 Dorinda Stream     Shaniquafort  South Carolina  39347-4438   \n",
      "4                           NaN              NaN             NaN         NaN   \n",
      "\n",
      "                 phone                          email           work  \\\n",
      "0   (180)940-9676x4495           shanna73@hotmail.com     Hahn-Mayer   \n",
      "1  (308)699-6239x81011            ferrell81@gmail.com  Buckridge Inc   \n",
      "2                  NaN                            NaN            NaN   \n",
      "3         869-985-6299  emmerich.griselda@hotmail.com    Witting PLC   \n",
      "4                  NaN                            NaN            NaN   \n",
      "\n",
      "                     work address       work city work state work zipcode  \\\n",
      "0  8177 Weber Throughway Apt. 341        Jaronton      Maine   51589-1424   \n",
      "1              236 Kessler Center  New Gavynshire   Missouri   52234-6972   \n",
      "2                             NaN             NaN        NaN          NaN   \n",
      "3               521 Kemmer Manors        Nerytown   Kentucky        68774   \n",
      "4                             NaN             NaN        NaN          NaN   \n",
      "\n",
      "          work phone                 work email   account created on  \n",
      "0        01240240340  heller.kirstin@glover.com  2001-09-06 06:15:24  \n",
      "1  (486)896-6855x446   esta.dicki@bechtelar.com           1983-04-25  \n",
      "2                NaN                        NaN                  NaN  \n",
      "3      (212)169-8190        greyson39@purdy.com  1971-04-27 14:05:06  \n",
      "4                NaN                        NaN                  NaN  \n"
     ]
    }
   ],
   "source": [
    "# Display the shape of the DataFrame (rows, columns)\n",
    "print(Custemers_Data.shape)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(Custemers_Data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the dataframe.columns to display the dictionary indexing your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['name', 'address', 'city', 'state', 'zip', 'phone', 'email', 'work',\n",
      "       'work address', 'work city', 'work state', 'work zipcode', 'work phone',\n",
      "       'work email', 'account created on'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Display column names\n",
    "print(Custemers_Data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Catch missing values\n",
    "Now we want to evaluate the number of missing data in the hole data. Display the total of missing values in each\n",
    "column of custemers Data\n",
    "1. You can use dataframe.isnull() and verctorization to sum missing values by each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                    81\n",
      "address               3365\n",
      "city                  3374\n",
      "state                 3365\n",
      "zip                   3362\n",
      "phone                 3366\n",
      "email                 3363\n",
      "work                  3390\n",
      "work address          3383\n",
      "work city             3378\n",
      "work state            3390\n",
      "work zipcode          3378\n",
      "work phone            3377\n",
      "work email            3359\n",
      "account created on    3368\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in each column\n",
    "missing_values = Custemers_Data.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Display the rows with missing values in the ’name’ column, you should have 81 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     name                          address              city           state  \\\n",
      "148   NaN     558 Brycen Mission Suite 152        Cristmouth        Arkansas   \n",
      "273   NaN       0481 Sanford Lake Apt. 439     Bashirianberg  North Carolina   \n",
      "300   NaN    38689 Kimora Groves Suite 807    New Nadiahaven         Vermont   \n",
      "330   NaN       077 Walsh Summit Suite 123        Rogahnfurt         Indiana   \n",
      "467   NaN              87140 Loma Crescent   North Dixieport        Michigan   \n",
      "...   ...                              ...               ...             ...   \n",
      "9316  NaN               054 Aubrie Corners    East Genevieve   New Hampshire   \n",
      "9487  NaN       9957 Rempel Wells Apt. 081         New Micky         Alabama   \n",
      "9745  NaN  6277 Schneider Common Suite 939         Port Aura    North Dakota   \n",
      "9816  NaN               986 Brianne Shoals  Port Wilbertstad     Mississippi   \n",
      "9834  NaN             19233 Kreiger Meadow        Kleinville        Michigan   \n",
      "\n",
      "             zip               phone                            email  \\\n",
      "148   52585-7480    +03(6)2449148729               corene08@gmail.com   \n",
      "273   61948-9865  (967)713-7747x7329  hermiston.jenniffer@hotmail.com   \n",
      "300   99133-2546  595.341.7775x50639               sfritsch@yahoo.com   \n",
      "330        81369    +38(1)3931574807             tdenesik@hotmail.com   \n",
      "467   32196-8397    +65(0)2946604451              lucille69@gmail.com   \n",
      "...          ...                 ...                              ...   \n",
      "9316       92371    296-242-9406x586         enrico.kuvalis@yahoo.com   \n",
      "9487       24013        522-665-0735              rylan47@hotmail.com   \n",
      "9745       10127        127.858.7550                 kacy29@gmail.com   \n",
      "9816       99396    953.475.7297x373         ritchie.hadley@yahoo.com   \n",
      "9834  42537-0382   (894)710-7706x071            hjalmer14@hotmail.com   \n",
      "\n",
      "                   work                  work address         work city  \\\n",
      "148           Braun PLC    8795 Carissa Land Apt. 884  South Arachester   \n",
      "273           Grant LLC                7375 Lynn Fork      Wuckerthaven   \n",
      "300       Purdy-Farrell              29365 Nyah Flats     West Isabella   \n",
      "330        Corwin-Fahey  717 Karol Stravenue Apt. 964      Trantowhaven   \n",
      "467         Monahan Ltd         99220 Murphy Motorway     East Reyhaven   \n",
      "...                 ...                           ...               ...   \n",
      "9316      Schneider LLC          53056 Gaige Motorway    North Audriana   \n",
      "9487  Bergnaum and Sons   60120 Klocko Alley Apt. 615       Adalineport   \n",
      "9745         Becker PLC           186 Armstrong Lakes      East Rosalyn   \n",
      "9816       Kautzer-Batz  22243 Feil Terrace Suite 469      New Catofurt   \n",
      "9834        Beer-Ledner            48801 Bobbye Roads        West Tylor   \n",
      "\n",
      "        work state work zipcode          work phone  \\\n",
      "148     New Mexico   68070-4086         09359769734   \n",
      "273           Ohio   89342-4989        419-132-5389   \n",
      "300        Florida        06342    +66(4)7117753075   \n",
      "330           Utah   49873-6641   882.577.0021x2495   \n",
      "467   Rhode Island        50641        773.245.8906   \n",
      "...            ...          ...                 ...   \n",
      "9316      Arkansas        73816        605.101.0879   \n",
      "9487      Maryland   37182-8209   289.393.5802x9949   \n",
      "9745          Ohio        07576  1-924-436-1559x850   \n",
      "9816     Wisconsin        62178         09723145605   \n",
      "9834      Arkansas        60338    849.567.9201x682   \n",
      "\n",
      "                              work email   account created on  \n",
      "148   hezzie.hettinger@mcglynnlarson.com           1999-12-12  \n",
      "273                 colin.walsh@hane.com  1995-08-07 19:52:30  \n",
      "300                     ocrooks@mohr.net           1975-04-07  \n",
      "330          mschaden@gerholdschultz.net           03/10/2010  \n",
      "467            parisian.willard@feil.org           04/29/1986  \n",
      "...                                  ...                  ...  \n",
      "9316         cephus.conroy@schroeder.com           01/21/2000  \n",
      "9487                    mwolf@corwin.net           2000-09-07  \n",
      "9745         kunde.carry@auerweimann.org  1995-08-13 11:03:45  \n",
      "9816           thompson.eulalie@mann.com  1970-01-27 16:26:56  \n",
      "9834            hsatterfield@bradtke.com  2015-10-13 23:24:40  \n",
      "\n",
      "[81 rows x 15 columns]\n",
      "Number of rows with missing names: 81\n"
     ]
    }
   ],
   "source": [
    "# Display rows where 'name' column has missing values\n",
    "missing_names = Custemers_Data[Custemers_Data['name'].isnull()]\n",
    "print(missing_names)\n",
    "\n",
    "# Check the number of rows with missing names\n",
    "print(f\"Number of rows with missing names: {missing_names.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Try removing rows with missing names. We call dropna() function which has many arguments to scale the\n",
    "level of a filtered data:\n",
    "• axis=0 or 1 allows to filter the missing values according to the row(axis=0) or the column(axis=1)\n",
    "• how=all allows to eliminate all rows or all columns having mainly missing values\n",
    "• inplace = True allow to need the same dataframe without creating a new output\n",
    "• thresh= integer value allows to keep only rows (or columns) having a missing value rate more than\n",
    "thresh.\n",
    "test the function dropna() on your dataframe as:\n",
    "• dropna(how=’all’, inplace=True) and check if missing values are removed!\n",
    "• dropna(inplace=True, axis=0) and check if missing values are removed!\n",
    "What did you conclude?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                    81\n",
      "address               3365\n",
      "city                  3374\n",
      "state                 3365\n",
      "zip                   3362\n",
      "phone                 3366\n",
      "email                 3363\n",
      "work                  3390\n",
      "work address          3383\n",
      "work city             3378\n",
      "work state            3390\n",
      "work zipcode          3378\n",
      "work phone            3377\n",
      "work email            3359\n",
      "account created on    3368\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "Custemers_Data.dropna(how='all', inplace=True)\n",
    "# Check if any rows with all missing values were removed\n",
    "print(Custemers_Data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                  0\n",
      "address               0\n",
      "city                  0\n",
      "state                 0\n",
      "zip                   0\n",
      "phone                 0\n",
      "email                 0\n",
      "work                  0\n",
      "work address          0\n",
      "work city             0\n",
      "work state            0\n",
      "work zipcode          0\n",
      "work phone            0\n",
      "work email            0\n",
      "account created on    0\n",
      "dtype: int64"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Custemers_Data.dropna(inplace=True, axis=0)\n",
    "# Check if rows with any missing values were removed\n",
    "print(Custemers_Data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use dropna(how='all'), it will only remove rows that are entirely made up of NaN values. If a row has even one non-NaN value, it will not be removed.\n",
    "When you use dropna(axis=0), it removes all rows that contain any missing value. If you had missing values scattered throughout the DataFrame, this would remove a significant number of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Ensure that values have the right format/type\n",
    "If we take the ”name” column, we find the first name and the last name separated with a blank character\n",
    "1. check the type of each column using the function dtype()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                  object\n",
      "address               object\n",
      "city                  object\n",
      "state                 object\n",
      "zip                   object\n",
      "phone                 object\n",
      "email                 object\n",
      "work                  object\n",
      "work address          object\n",
      "work city             object\n",
      "work state            object\n",
      "work zipcode          object\n",
      "work phone            object\n",
      "work email            object\n",
      "account created on    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check the data type of each column\n",
    "print(Custemers_Data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Let we split the column ’name’ into two columns where the first string is the first name and the second\n",
    "string is the last name. (see str.split()).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  first_name        last_name\n",
      "0     Deonte            Stark\n",
      "1   Faustino            Boyer\n",
      "3     Mervyn          Kreiger\n",
      "5     Daquan          Leffler\n",
      "6             Jace Konopelski\n"
     ]
    }
   ],
   "source": [
    "# Split 'name' column into 'first_name' and 'last_name' columns\n",
    "# Check for missing values in the 'name' column\n",
    "names_split = Custemers_Data['name'].str.split(' ', n=1, expand=True)  # n=1 to limit to two parts\n",
    "Custemers_Data['first_name'] = names_split[0]\n",
    "Custemers_Data['last_name'] = names_split[1].fillna('')  # Fill NaN with empty string for last_name\n",
    "print(Custemers_Data[['first_name', 'last_name']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. You can remark incorrect lastname values. Remove rows having lastname length up to 16 characters.\n",
    "Take 5 mn to outline the main ideas you’ve retained from these exercises (Part I and II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5366, 17)\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where 'last_name' has more than 16 characters\n",
    "Custemers_Data = Custemers_Data[Custemers_Data['last_name'].str.len() <= 16]\n",
    "print(Custemers_Data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the key ideas retained from these exercises:\n",
    "\n",
    "Data Management with Pandas: You learned how to import and load data from a CSV file into a Pandas DataFrame, how to check the structure of the DataFrame (using functions like shape(), head(), columns), and how to query data effectively.\n",
    "Handling Missing Data: You gained experience using Pandas functions like isnull(), dropna(), and filtering data based on missing values. We also explored different ways to handle missing data, like dropping rows or columns with missing values.\n",
    "String Operations: You practiced string manipulations such as splitting strings within a column into multiple columns and cleaning up data by handling incorrect values (e.g., removing rows with long last_name values).\n",
    "Vectorization vs Loops: You learned the benefits of vectorized operations, especially for performance, and how they compare to loops.\n",
    "Data Integrity: Ensuring that the data types and formats of columns are correct is essential for any further analysis or preprocessing steps in the machine learning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Data repairing with imputation\n",
    "## Exrcise 6: Do it from scratch at home and upload it on Moodle\n",
    "Now you have new datasets ”Olympics.csv” and ”flicker.csv” available on Moodle. You have to define your own\n",
    "strategy to clean these data and comment your notebook at each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flicker Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8287 entries, 0 to 8286\n",
      "Data columns (total 15 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Identifier              8287 non-null   int64  \n",
      " 1   Edition Statement       773 non-null    object \n",
      " 2   Place of Publication    8287 non-null   object \n",
      " 3   Date of Publication     8106 non-null   object \n",
      " 4   Publisher               4092 non-null   object \n",
      " 5   Title                   8287 non-null   object \n",
      " 6   Author                  6509 non-null   object \n",
      " 7   Contributors            8287 non-null   object \n",
      " 8   Corporate Author        0 non-null      float64\n",
      " 9   Corporate Contributors  0 non-null      float64\n",
      " 10  Former owner            1 non-null      object \n",
      " 11  Engraver                0 non-null      float64\n",
      " 12  Issuance type           8287 non-null   object \n",
      " 13  Flickr URL              8287 non-null   object \n",
      " 14  Shelfmarks              8287 non-null   object \n",
      "dtypes: float64(3), int64(1), object(11)\n",
      "memory usage: 971.3+ KB\n",
      "\n",
      "Missing values in Flicker Dataset:\n",
      "Identifier                   0\n",
      "Edition Statement         7514\n",
      "Place of Publication         0\n",
      "Date of Publication        181\n",
      "Publisher                 4195\n",
      "Title                        0\n",
      "Author                    1778\n",
      "Contributors                 0\n",
      "Corporate Author          8287\n",
      "Corporate Contributors    8287\n",
      "Former owner              8286\n",
      "Engraver                  8287\n",
      "Issuance type                0\n",
      "Flickr URL                   0\n",
      "Shelfmarks                   0\n",
      "dtype: int64\n",
      "\n",
      "Olympics Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 148 entries, 0 to 147\n",
      "Data columns (total 16 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       147 non-null    object\n",
      " 1   1       148 non-null    object\n",
      " 2   2       148 non-null    object\n",
      " 3   3       148 non-null    object\n",
      " 4   4       148 non-null    object\n",
      " 5   5       148 non-null    object\n",
      " 6   6       148 non-null    object\n",
      " 7   7       148 non-null    object\n",
      " 8   8       148 non-null    object\n",
      " 9   9       148 non-null    object\n",
      " 10  10      148 non-null    object\n",
      " 11  11      148 non-null    object\n",
      " 12  12      148 non-null    object\n",
      " 13  13      148 non-null    object\n",
      " 14  14      148 non-null    object\n",
      " 15  15      148 non-null    object\n",
      "dtypes: object(16)\n",
      "memory usage: 18.6+ KB\n",
      "\n",
      "Missing values in Olympics Dataset:\n",
      "0     1\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "5     0\n",
      "6     0\n",
      "7     0\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "11    0\n",
      "12    0\n",
      "13    0\n",
      "14    0\n",
      "15    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "import pandas as pd\n",
    "\n",
    "flickers_df = pd.read_csv('data/flicker.csv')\n",
    "olympics_df = pd.read_csv('data/olympics.csv')\n",
    "\n",
    "# Inspect Flicker dataset\n",
    "print(\"Flicker Dataset Info:\")\n",
    "flickers_df.info()\n",
    "\n",
    "# Check for missing values in the Flicker dataset\n",
    "print(\"\\nMissing values in Flicker Dataset:\")\n",
    "print(flickers_df.isnull().sum())\n",
    "\n",
    "# Inspect Olympics dataset\n",
    "print(\"\\nOlympics Dataset Info:\")\n",
    "olympics_df.info()\n",
    "\n",
    "# Check for missing values in the Olympics dataset\n",
    "print(\"\\nMissing values in Olympics Dataset:\")\n",
    "print(olympics_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flicker Dataset Info After Cleaning:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8287 entries, 0 to 8286\n",
      "Data columns (total 11 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   Identifier            8287 non-null   int64 \n",
      " 1   Edition Statement     8287 non-null   object\n",
      " 2   Place of Publication  8287 non-null   object\n",
      " 3   Date of Publication   8106 non-null   object\n",
      " 4   Publisher             4092 non-null   object\n",
      " 5   Title                 8287 non-null   object\n",
      " 6   Author                6509 non-null   object\n",
      " 7   Contributors          8287 non-null   object\n",
      " 8   Issuance type         8287 non-null   object\n",
      " 9   Flickr URL            8287 non-null   object\n",
      " 10  Shelfmarks            8287 non-null   object\n",
      "dtypes: int64(1), object(10)\n",
      "memory usage: 712.3+ KB\n",
      "\n",
      "Olympics Dataset Info After Cleaning:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 147 entries, 1 to 147\n",
      "Data columns (total 16 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   Country              147 non-null    object\n",
      " 1   Summer Games         147 non-null    int64 \n",
      " 2   Summer Gold          147 non-null    int64 \n",
      " 3   Summer Silver        147 non-null    int64 \n",
      " 4   Summer Bronze        147 non-null    int64 \n",
      " 5   Total Summer Medals  147 non-null    int64 \n",
      " 6   Winter Games         147 non-null    int64 \n",
      " 7   Winter Gold          147 non-null    int64 \n",
      " 8   Winter Silver        147 non-null    int64 \n",
      " 9   Winter Bronze        147 non-null    int64 \n",
      " 10  Total Winter Medals  147 non-null    int64 \n",
      " 11  Games Total          147 non-null    int64 \n",
      " 12  Gold Total           147 non-null    int64 \n",
      " 13  Silver Total         147 non-null    int64 \n",
      " 14  Bronze Total         147 non-null    int64 \n",
      " 15  Combined Total       147 non-null    int64 \n",
      "dtypes: int64(15), object(1)\n",
      "memory usage: 18.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the Flicker Dataset\n",
    "# Dropping irrelevant or empty columns\n",
    "flickers_cleaned_df = flickers_df.drop(columns=['Corporate Author', 'Corporate Contributors', 'Former owner', 'Engraver'])\n",
    "\n",
    "# Filling missing 'Edition Statement' with 'Unknown'\n",
    "flickers_cleaned_df['Edition Statement'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Cleaning the Olympics Dataset\n",
    "# Renaming columns for clarity\n",
    "olympics_df.columns = [\n",
    "    'Country', 'Summer Games', 'Summer Gold', 'Summer Silver', 'Summer Bronze', 'Total Summer Medals', \n",
    "    'Winter Games', 'Winter Gold', 'Winter Silver', 'Winter Bronze', 'Total Winter Medals', \n",
    "    'Games Total', 'Gold Total', 'Silver Total', 'Bronze Total', 'Combined Total'\n",
    "]\n",
    "\n",
    "# Removing the first row (header descriptions)\n",
    "olympics_cleaned_df = olympics_df.drop(index=0)\n",
    "\n",
    "# Converting numeric columns to correct types\n",
    "numeric_columns = [\n",
    "    'Summer Games', 'Summer Gold', 'Summer Silver', 'Summer Bronze', 'Total Summer Medals', \n",
    "    'Winter Games', 'Winter Gold', 'Winter Silver', 'Winter Bronze', 'Total Winter Medals', \n",
    "    'Games Total', 'Gold Total', 'Silver Total', 'Bronze Total', 'Combined Total'\n",
    "]\n",
    "\n",
    "olympics_cleaned_df[numeric_columns] = olympics_cleaned_df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Checking cleaned data\n",
    "print(\"Flicker Dataset Info After Cleaning:\")\n",
    "flickers_cleaned_df.info()\n",
    "\n",
    "print(\"\\nOlympics Dataset Info After Cleaning:\")\n",
    "olympics_cleaned_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Flicker Dataset Info After Handling Missing Data:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8287 entries, 0 to 8286\n",
      "Data columns (total 11 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   Identifier            8287 non-null   int64 \n",
      " 1   Edition Statement     8287 non-null   object\n",
      " 2   Place of Publication  8287 non-null   object\n",
      " 3   Date of Publication   8287 non-null   object\n",
      " 4   Publisher             8287 non-null   object\n",
      " 5   Title                 8287 non-null   object\n",
      " 6   Author                8287 non-null   object\n",
      " 7   Contributors          8287 non-null   object\n",
      " 8   Issuance type         8287 non-null   object\n",
      " 9   Flickr URL            8287 non-null   object\n",
      " 10  Shelfmarks            8287 non-null   object\n",
      "dtypes: int64(1), object(10)\n",
      "memory usage: 712.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Handling missing data in Flicker Dataset\n",
    "flickers_cleaned_df['Publisher'].fillna('Unknown Publisher', inplace=True)\n",
    "flickers_cleaned_df['Author'].fillna('Unknown Author', inplace=True)\n",
    "flickers_cleaned_df['Date of Publication'].fillna('Unknown Year', inplace=True)\n",
    "\n",
    "# Olympics Dataset doesn't have missing values, so no action required.\n",
    "\n",
    "# Checking final state after handling missing data\n",
    "print(\"Final Flicker Dataset Info After Handling Missing Data:\")\n",
    "flickers_cleaned_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Country Names in Olympics Dataset:\n",
      "['Afghanistan\\xa0(AFG)' 'Algeria\\xa0(ALG)' 'Argentina\\xa0(ARG)'\n",
      " 'Armenia\\xa0(ARM)' 'Australasia\\xa0(ANZ) [ANZ]'\n",
      " 'Australia\\xa0(AUS) [AUS] [Z]' 'Austria\\xa0(AUT)' 'Azerbaijan\\xa0(AZE)'\n",
      " 'Bahamas\\xa0(BAH)' 'Bahrain\\xa0(BRN)' 'Barbados\\xa0(BAR) [BAR]'\n",
      " 'Belarus\\xa0(BLR)' 'Belgium\\xa0(BEL)' 'Bermuda\\xa0(BER)'\n",
      " 'Bohemia\\xa0(BOH) [BOH] [Z]' 'Botswana\\xa0(BOT)' 'Brazil\\xa0(BRA)'\n",
      " 'British West Indies\\xa0(BWI) [BWI]' 'Bulgaria\\xa0(BUL) [H]'\n",
      " 'Burundi\\xa0(BDI)' 'Cameroon\\xa0(CMR)' 'Canada\\xa0(CAN)'\n",
      " 'Chile\\xa0(CHI) [I]' 'China\\xa0(CHN) [CHN]' 'Colombia\\xa0(COL)'\n",
      " 'Costa Rica\\xa0(CRC)' 'Ivory Coast\\xa0(CIV) [CIV]' 'Croatia\\xa0(CRO)'\n",
      " 'Cuba\\xa0(CUB) [Z]' 'Cyprus\\xa0(CYP)' 'Czech Republic\\xa0(CZE) [CZE]'\n",
      " 'Czechoslovakia\\xa0(TCH) [TCH]' 'Denmark\\xa0(DEN) [Z]'\n",
      " 'Djibouti\\xa0(DJI) [B]' 'Dominican Republic\\xa0(DOM)' 'Ecuador\\xa0(ECU)'\n",
      " 'Egypt\\xa0(EGY) [EGY] [Z]' 'Eritrea\\xa0(ERI)' 'Estonia\\xa0(EST)'\n",
      " 'Ethiopia\\xa0(ETH)' 'Finland\\xa0(FIN)' 'France\\xa0(FRA) [O] [P] [Z]'\n",
      " 'Gabon\\xa0(GAB)' 'Georgia\\xa0(GEO)' 'Germany\\xa0(GER) [GER] [Z]'\n",
      " 'United Team of Germany\\xa0(EUA) [EUA]' 'East Germany\\xa0(GDR) [GDR]'\n",
      " 'West Germany\\xa0(FRG) [FRG]' 'Ghana\\xa0(GHA) [GHA]'\n",
      " 'Great Britain\\xa0(GBR) [GBR] [Z]' 'Greece\\xa0(GRE) [Z]'\n",
      " 'Grenada\\xa0(GRN)' 'Guatemala\\xa0(GUA)' 'Guyana\\xa0(GUY) [GUY]'\n",
      " 'Haiti\\xa0(HAI) [J]' 'Hong Kong\\xa0(HKG) [HKG]' 'Hungary\\xa0(HUN)'\n",
      " 'Iceland\\xa0(ISL)' 'India\\xa0(IND) [F]' 'Indonesia\\xa0(INA)'\n",
      " 'Iran\\xa0(IRI) [K]' 'Iraq\\xa0(IRQ)' 'Ireland\\xa0(IRL)' 'Israel\\xa0(ISR)'\n",
      " 'Italy\\xa0(ITA) [M] [S]' 'Jamaica\\xa0(JAM) [JAM]' 'Japan\\xa0(JPN)'\n",
      " 'Kazakhstan\\xa0(KAZ)' 'Kenya\\xa0(KEN)' 'North Korea\\xa0(PRK)'\n",
      " 'South Korea\\xa0(KOR)' 'Kuwait\\xa0(KUW)' 'Kyrgyzstan\\xa0(KGZ)'\n",
      " 'Latvia\\xa0(LAT)' 'Lebanon\\xa0(LIB)' 'Liechtenstein\\xa0(LIE)'\n",
      " 'Lithuania\\xa0(LTU)' 'Luxembourg\\xa0(LUX) [O]' 'Macedonia\\xa0(MKD)'\n",
      " 'Malaysia\\xa0(MAS) [MAS]' 'Mauritius\\xa0(MRI)' 'Mexico\\xa0(MEX)'\n",
      " 'Moldova\\xa0(MDA)' 'Mongolia\\xa0(MGL)' 'Montenegro\\xa0(MNE)'\n",
      " 'Morocco\\xa0(MAR)' 'Mozambique\\xa0(MOZ)' 'Namibia\\xa0(NAM)'\n",
      " 'Netherlands\\xa0(NED) [Z]' 'Netherlands Antilles\\xa0(AHO) [AHO] [I]'\n",
      " 'New Zealand\\xa0(NZL) [NZL]' 'Niger\\xa0(NIG)' 'Nigeria\\xa0(NGR)'\n",
      " 'Norway\\xa0(NOR) [Q]' 'Pakistan\\xa0(PAK)' 'Panama\\xa0(PAN)'\n",
      " 'Paraguay\\xa0(PAR)' 'Peru\\xa0(PER) [L]' 'Philippines\\xa0(PHI)'\n",
      " 'Poland\\xa0(POL)' 'Portugal\\xa0(POR)' 'Puerto Rico\\xa0(PUR)'\n",
      " 'Qatar\\xa0(QAT)' 'Romania\\xa0(ROU)' 'Russia\\xa0(RUS) [RUS]'\n",
      " 'Russian Empire\\xa0(RU1) [RU1]' 'Soviet Union\\xa0(URS) [URS]'\n",
      " 'Unified Team\\xa0(EUN) [EUN]' 'Saudi Arabia\\xa0(KSA)' 'Senegal\\xa0(SEN)'\n",
      " 'Serbia\\xa0(SRB) [SRB]' 'Serbia and Montenegro\\xa0(SCG) [SCG]'\n",
      " 'Singapore\\xa0(SIN)' 'Slovakia\\xa0(SVK) [SVK]' 'Slovenia\\xa0(SLO)'\n",
      " 'South Africa\\xa0(RSA)' 'Spain\\xa0(ESP) [Z]' 'Sri Lanka\\xa0(SRI) [SRI]'\n",
      " 'Sudan\\xa0(SUD)' 'Suriname\\xa0(SUR) [E]' 'Sweden\\xa0(SWE) [Z]'\n",
      " 'Switzerland\\xa0(SUI)' 'Syria\\xa0(SYR)'\n",
      " 'Chinese Taipei\\xa0(TPE) [TPE] [TPE2]' 'Tajikistan\\xa0(TJK)'\n",
      " 'Tanzania\\xa0(TAN) [TAN]' 'Thailand\\xa0(THA)' 'Togo\\xa0(TOG)'\n",
      " 'Tonga\\xa0(TGA)' 'Trinidad and Tobago\\xa0(TRI) [TRI]' 'Tunisia\\xa0(TUN)'\n",
      " 'Turkey\\xa0(TUR)' 'Uganda\\xa0(UGA)' 'Ukraine\\xa0(UKR)'\n",
      " 'United Arab Emirates\\xa0(UAE)' 'United States\\xa0(USA) [P] [Q] [R] [Z]'\n",
      " 'Uruguay\\xa0(URU)' 'Uzbekistan\\xa0(UZB)' 'Venezuela\\xa0(VEN)'\n",
      " 'Vietnam\\xa0(VIE)' 'Virgin Islands\\xa0(ISV)' 'Yugoslavia\\xa0(YUG) [YUG]'\n",
      " 'Independent Olympic Participants\\xa0(IOP) [IOP]' 'Zambia\\xa0(ZAM) [ZAM]'\n",
      " 'Zimbabwe\\xa0(ZIM) [ZIM]' 'Mixed team\\xa0(ZZX) [ZZX]' 'Totals']\n",
      "\n",
      "Sample Flicker Dataset (After Normalization):\n",
      "  Date of Publication     Author              Publisher  \\\n",
      "0                1879      A. A.       S. Tinsley & Co.   \n",
      "1                1868  A., A. A.           Virtue & Co.   \n",
      "2                1869  A., A. A.  Bradbury, Evans & Co.   \n",
      "3                1851  A., E. S.          James Darling   \n",
      "4                1857  A., E. S.   Wertheim & Macintosh   \n",
      "\n",
      "       Place of Publication  \n",
      "0                    London  \n",
      "1  London; Virtue & Yorston  \n",
      "2                    London  \n",
      "3                    London  \n",
      "4                    London  \n",
      "\n",
      "Sample Olympics Dataset (After Normalization):\n",
      "                   Country\n",
      "1        Afghanistan (AFG)\n",
      "2            Algeria (ALG)\n",
      "3          Argentina (ARG)\n",
      "4            Armenia (ARM)\n",
      "5  Australasia (ANZ) [ANZ]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: Normalizing and Formatting Data\n",
    "\n",
    "# 1. For the Flicker Dataset\n",
    "# Normalize 'Date of Publication' (extract year and clean inconsistent data)\n",
    "# Example: Removing non-numeric characters and retaining the year only\n",
    "flickers_cleaned_df['Date of Publication'] = flickers_cleaned_df['Date of Publication'].str.extract(r'(\\d{4})')\n",
    "\n",
    "# Strip any leading/trailing whitespaces in 'Author', 'Publisher', and 'Place of Publication'\n",
    "flickers_cleaned_df['Author'] = flickers_cleaned_df['Author'].str.strip()\n",
    "flickers_cleaned_df['Publisher'] = flickers_cleaned_df['Publisher'].str.strip()\n",
    "flickers_cleaned_df['Place of Publication'] = flickers_cleaned_df['Place of Publication'].str.strip()\n",
    "\n",
    "# 2. For the Olympics Dataset\n",
    "# Standardizing country names if needed (strip leading/trailing spaces, handle inconsistencies)\n",
    "olympics_cleaned_df['Country'] = olympics_cleaned_df['Country'].str.strip()\n",
    "\n",
    "# Verifying the unique country names to detect inconsistencies\n",
    "print(\"Unique Country Names in Olympics Dataset:\")\n",
    "print(olympics_cleaned_df['Country'].unique())\n",
    "\n",
    "# Display the cleaned columns for checking\n",
    "print(\"\\nSample Flicker Dataset (After Normalization):\")\n",
    "print(flickers_cleaned_df[['Date of Publication', 'Author', 'Publisher', 'Place of Publication']].head())\n",
    "\n",
    "print(\"\\nSample Olympics Dataset (After Normalization):\")\n",
    "print(olympics_cleaned_df[['Country']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in Flicker Dataset after cleaning:\n",
      "Identifier                0\n",
      "Edition Statement         0\n",
      "Place of Publication      0\n",
      "Date of Publication     183\n",
      "Publisher                 0\n",
      "Title                     0\n",
      "Author                    0\n",
      "Contributors              0\n",
      "Issuance type             0\n",
      "Flickr URL                0\n",
      "Shelfmarks                0\n",
      "dtype: int64\n",
      "\n",
      "Unique values in 'Date of Publication' (should only be years or NaN):\n",
      "['1879' '1868' '1869' '1851' '1857' '1875' '1872' nan '1676' '1679' '1802'\n",
      " '1859' '1888' '1839' '1897' '1865' '1860' '1873' '1866' '1899' '1814'\n",
      " '1820' '1800' '1847' '1893' '1805' '1837' '1896' '1898' '1892' '1894'\n",
      " '1885' '1846' '1817' '1816' '1833' '1804' '1777' '1799' '1827' '1853'\n",
      " '1874' '1790' '1883' '1795' '1877' '1886' '1834' '1852' '1828' '1876'\n",
      " '1758' '1880' '1823' '1887' '1825' '1850' '1810' '1889' '1861' '1858'\n",
      " '1878' '1821' '1891' '1808' '1849' '1724' '1772' '1812' '1835' '1867'\n",
      " '1830' '1841' '1884' '1863' '1848' '1845' '1807' '1864' '1822' '1871'\n",
      " '1829' '1824' '1856' '1803' '1818' '1881' '1838' '1720' '1855' '1747'\n",
      " '1748' '1704' '1762' '1694' '1890' '1895' '1870' '1809' '1760' '1862'\n",
      " '1844' '1813' '1787' '1794' '1710' '1785' '1801' '1882' '1840' '1819'\n",
      " '1651' '1842' '1744' '1728' '1806' '1831' '1779' '1836' '1756' '1843'\n",
      " '1792' '1854' '1731' '1770' '1734' '1775' '1826' '1640' '1687' '1778'\n",
      " '1695' '1749' '1782' '1690' '1696' '1768' '1716' '1774' '1811' '1796'\n",
      " '1798' '1773' '1788' '1607' '1657' '1635' '1815' '1759' '1683' '1709'\n",
      " '1733' '1832' '1784' '1625' '1639' '1737' '1718' '1671' '1684' '1662'\n",
      " '1765' '1705' '1781' '1693' '1701' '1776' '1675' '1767' '1654' '1780'\n",
      " '1766' '1732' '1786' '1703' '1791' '1661' '1678' '1751' '1764' '1677'\n",
      " '1739' '1769' '1735' '1757' '1691' '1717' '1753' '1746' '1685' '1670'\n",
      " '1761' '1570' '1721' '1540' '1633' '1752' '1699' '1648' '1719' '1656'\n",
      " '1793' '1673' '1915' '1631' '1725' '1797' '1698' '1647' '1663' '1692'\n",
      " '1783' '1713' '1689' '1736' '1700' '1789' '1729' '1743' '1658' '1632'\n",
      " '1602' '1641' '1742' '1653' '1763' '1686' '1771' '1730' '1707' '1660'\n",
      " '1674' '1911' '1754' '1738' '1510' '1722' '1740' '1664' '1638' '1682'\n",
      " '1667' '1900' '1750' '1681' '1755' '1688' '1741' '1672' '1628' '1680'\n",
      " '1702' '1637' '1592' '1697' '1708']\n",
      "\n",
      "Checking for duplicates in Flicker Dataset (based on Identifier):\n",
      "0\n",
      "\n",
      "Missing values in Olympics Dataset after cleaning:\n",
      "Country                0\n",
      "Summer Games           0\n",
      "Summer Gold            0\n",
      "Summer Silver          0\n",
      "Summer Bronze          0\n",
      "Total Summer Medals    0\n",
      "Winter Games           0\n",
      "Winter Gold            0\n",
      "Winter Silver          0\n",
      "Winter Bronze          0\n",
      "Total Winter Medals    0\n",
      "Games Total            0\n",
      "Gold Total             0\n",
      "Silver Total           0\n",
      "Bronze Total           0\n",
      "Combined Total         0\n",
      "dtype: int64\n",
      "\n",
      "Olympics Dataset column types:\n",
      "Country                object\n",
      "Summer Games            int64\n",
      "Summer Gold             int64\n",
      "Summer Silver           int64\n",
      "Summer Bronze           int64\n",
      "Total Summer Medals     int64\n",
      "Winter Games            int64\n",
      "Winter Gold             int64\n",
      "Winter Silver           int64\n",
      "Winter Bronze           int64\n",
      "Total Winter Medals     int64\n",
      "Games Total             int64\n",
      "Gold Total              int64\n",
      "Silver Total            int64\n",
      "Bronze Total            int64\n",
      "Combined Total          int64\n",
      "dtype: object\n",
      "\n",
      "Checking if 'Gold Total' + 'Silver Total' + 'Bronze Total' matches 'Combined Total':\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5: Validating the Cleaned Data\n",
    "\n",
    "# 1. For the Flicker Dataset\n",
    "# Check for any new missing values after normalization\n",
    "print(\"Missing values in Flicker Dataset after cleaning:\")\n",
    "print(flickers_cleaned_df.isnull().sum())\n",
    "\n",
    "# Ensure that the 'Date of Publication' column now contains only valid years or NaN\n",
    "print(\"\\nUnique values in 'Date of Publication' (should only be years or NaN):\")\n",
    "print(flickers_cleaned_df['Date of Publication'].unique())\n",
    "\n",
    "# Check for duplicate entries based on key fields like 'Identifier'\n",
    "print(\"\\nChecking for duplicates in Flicker Dataset (based on Identifier):\")\n",
    "print(flickers_cleaned_df['Identifier'].duplicated().sum())\n",
    "\n",
    "# 2. For the Olympics Dataset\n",
    "# Check for any new missing values in the cleaned dataset\n",
    "print(\"\\nMissing values in Olympics Dataset after cleaning:\")\n",
    "print(olympics_cleaned_df.isnull().sum())\n",
    "\n",
    "# Check that all numeric columns are correctly typed and contain no missing values\n",
    "print(\"\\nOlympics Dataset column types:\")\n",
    "print(olympics_cleaned_df.dtypes)\n",
    "\n",
    "# Verifying logical consistency: e.g., 'Gold Total' + 'Silver Total' + 'Bronze Total' == 'Combined Total'\n",
    "olympics_cleaned_df['Medal Total Check'] = (\n",
    "    olympics_cleaned_df['Gold Total'] + olympics_cleaned_df['Silver Total'] + olympics_cleaned_df['Bronze Total']\n",
    ")\n",
    "print(\"\\nChecking if 'Gold Total' + 'Silver Total' + 'Bronze Total' matches 'Combined Total':\")\n",
    "print((olympics_cleaned_df['Combined Total'] == olympics_cleaned_df['Medal Total Check']).all())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7: Imputation to handle missing data\n",
    "In this exercise we will compare two solutions of data cleaning. The first one consists to drop missing columns values while the second is to fill missing values with interesting ones. In your notebook, open the file melb data.csv.\n",
    "1. load your data as a dataframe and use a clear name as ”initialData”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "initialData = pd.read_csv('data/melb_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. observe first the quality of the data by using these functions: dataframe.columns, dataframe.info(), and\n",
    "dataframe.shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Suburb', 'Address', 'Rooms', 'Type', 'Price', 'Method',\n",
      "       'SellerG', 'Date', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom',\n",
      "       'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'CouncilArea',\n",
      "       'Lattitude', 'Longtitude', 'Regionname', 'Propertycount'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18396 entries, 0 to 18395\n",
      "Data columns (total 22 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Unnamed: 0     18396 non-null  int64  \n",
      " 1   Suburb         18396 non-null  object \n",
      " 2   Address        18396 non-null  object \n",
      " 3   Rooms          18396 non-null  int64  \n",
      " 4   Type           18396 non-null  object \n",
      " 5   Price          18396 non-null  float64\n",
      " 6   Method         18396 non-null  object \n",
      " 7   SellerG        18396 non-null  object \n",
      " 8   Date           18396 non-null  object \n",
      " 9   Distance       18395 non-null  float64\n",
      " 10  Postcode       18395 non-null  float64\n",
      " 11  Bedroom2       14927 non-null  float64\n",
      " 12  Bathroom       14925 non-null  float64\n",
      " 13  Car            14820 non-null  float64\n",
      " 14  Landsize       13603 non-null  float64\n",
      " 15  BuildingArea   7762 non-null   float64\n",
      " 16  YearBuilt      8958 non-null   float64\n",
      " 17  CouncilArea    12233 non-null  object \n",
      " 18  Lattitude      15064 non-null  float64\n",
      " 19  Longtitude     15064 non-null  float64\n",
      " 20  Regionname     18395 non-null  object \n",
      " 21  Propertycount  18395 non-null  float64\n",
      "dtypes: float64(12), int64(2), object(8)\n",
      "memory usage: 3.1+ MB\n",
      "None\n",
      "(18396, 22)\n"
     ]
    }
   ],
   "source": [
    "# Display the column names\n",
    "print(initialData.columns)\n",
    "\n",
    "# Display info about data types and non-null counts\n",
    "print(initialData.info())\n",
    "\n",
    "# Display the shape (rows, columns) of the DataFrame\n",
    "print(initialData.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. display the total missing values by columns (axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0           0\n",
      "Suburb               0\n",
      "Address              0\n",
      "Rooms                0\n",
      "Type                 0\n",
      "Price                0\n",
      "Method               0\n",
      "SellerG              0\n",
      "Date                 0\n",
      "Distance             1\n",
      "Postcode             1\n",
      "Bedroom2          3469\n",
      "Bathroom          3471\n",
      "Car               3576\n",
      "Landsize          4793\n",
      "BuildingArea     10634\n",
      "YearBuilt         9438\n",
      "CouncilArea       6163\n",
      "Lattitude         3332\n",
      "Longtitude        3332\n",
      "Regionname           1\n",
      "Propertycount        1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Total missing values by columns\n",
    "missing_by_columns = initialData.isnull().sum()\n",
    "print(missing_by_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. display total missing Values by rows (axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        2\n",
      "1        0\n",
      "2        0\n",
      "3        2\n",
      "4        0\n",
      "        ..\n",
      "18391    2\n",
      "18392    1\n",
      "18393    4\n",
      "18394    1\n",
      "18395    2\n",
      "Length: 18396, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Total missing values by rows\n",
    "missing_by_rows = initialData.isnull().sum(axis=1)\n",
    "print(missing_by_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. list the columns with missing values and store them in a variable colsWithMissing. Use the function isnull()\n",
    "and try to write a simple function you can call it with other datasets with the following signature and core\n",
    "code as following:\n",
    "If those columns had relevant information your model loses access to it when the column is dropped. Another drawback to this solution is to miss to do the same droping on the test dataset where an error will\n",
    "occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values: ['Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'CouncilArea', 'Lattitude', 'Longtitude', 'Regionname', 'Propertycount']\n"
     ]
    }
   ],
   "source": [
    "def missing_columns(originDB):\n",
    "    # List of columns with missing values\n",
    "    colsWithMissing = [col for col in originDB.columns if originDB[col].isnull().any()]\n",
    "    \n",
    "    # Drop columns with missing values\n",
    "    reduced_original_data = originDB.drop(colsWithMissing, axis=1)\n",
    "    \n",
    "    return colsWithMissing, reduced_original_data\n",
    "\n",
    "# Call the function on initialData\n",
    "colsWithMissing, reduced_initialData = missing_columns(initialData)\n",
    "print(\"Columns with missing values:\", colsWithMissing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. display the rate of missing values by columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0        0.000000\n",
      "Suburb            0.000000\n",
      "Address           0.000000\n",
      "Rooms             0.000000\n",
      "Type              0.000000\n",
      "Price             0.000000\n",
      "Method            0.000000\n",
      "SellerG           0.000000\n",
      "Date              0.000000\n",
      "Distance          0.005436\n",
      "Postcode          0.005436\n",
      "Bedroom2         18.857360\n",
      "Bathroom         18.868232\n",
      "Car              19.439008\n",
      "Landsize         26.054577\n",
      "BuildingArea     57.806045\n",
      "YearBuilt        51.304631\n",
      "CouncilArea      33.501848\n",
      "Lattitude        18.112633\n",
      "Longtitude       18.112633\n",
      "Regionname        0.005436\n",
      "Propertycount     0.005436\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Percentage of missing values by columns\n",
    "missing_rate_columns = (initialData.isnull().sum() / len(initialData)) * 100\n",
    "print(missing_rate_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. do the same on the rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         9.090909\n",
      "1         0.000000\n",
      "2         0.000000\n",
      "3         9.090909\n",
      "4         0.000000\n",
      "           ...    \n",
      "18391     9.090909\n",
      "18392     4.545455\n",
      "18393    18.181818\n",
      "18394     4.545455\n",
      "18395     9.090909\n",
      "Length: 18396, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Percentage of missing values by rows\n",
    "missing_rate_rows = (initialData.isnull().sum(axis=1) / initialData.shape[1]) * 100\n",
    "print(missing_rate_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. remove rows whom the rate of missing values are > 5% from the origin data and store the result on a new\n",
    "dataframe variable named new Data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of newData: (8332, 22)\n"
     ]
    }
   ],
   "source": [
    "# Threshold for removing rows with more than 5% missing values\n",
    "threshold = 0.05 * initialData.shape[1]\n",
    "\n",
    "# Remove rows with missing value rate > 5%\n",
    "newData = initialData[initialData.isnull().sum(axis=1) <= threshold]\n",
    "print(\"Shape of newData:\", newData.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. call your function missing colums(originDB) on both original data and on new data obtained after removal\n",
    "rows. How do you explain the columns difference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values in initial data: ['Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'CouncilArea', 'Lattitude', 'Longtitude', 'Regionname', 'Propertycount']\n",
      "Columns with missing values in new data: ['BuildingArea', 'YearBuilt', 'CouncilArea']\n"
     ]
    }
   ],
   "source": [
    "# Call on initialData\n",
    "colsWithMissing_initial, reduced_initialData = missing_columns(initialData)\n",
    "\n",
    "# Call on newData after removing rows\n",
    "colsWithMissing_new, reduced_newData = missing_columns(newData)\n",
    "\n",
    "print(\"Columns with missing values in initial data:\", colsWithMissing_initial)\n",
    "print(\"Columns with missing values in new data:\", colsWithMissing_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.  fill the missing values with the mean price, so you have to: 1) Display the statistical description of the\n",
    "column Price using the function describe(), 2) then to calculate the mean value and 3) to fill the missing\n",
    "value with the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    8.332000e+03\n",
      "mean     1.063969e+06\n",
      "std      6.668554e+05\n",
      "min      8.500000e+04\n",
      "25%      6.200000e+05\n",
      "50%      8.800000e+05\n",
      "75%      1.315000e+06\n",
      "max      9.000000e+06\n",
      "Name: Price, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Statistical description of the 'Price' column\n",
    "print(newData['Price'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Price: 1063969.2016322613\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean of the 'Price' column\n",
    "price_mean = newData['Price'].mean()\n",
    "print(\"Mean Price:\", price_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_23480\\3320696925.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  newData['Price'].fillna(price_mean, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values in 'Price' with the mean value\n",
    "newData['Price'].fillna(price_mean, inplace=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
