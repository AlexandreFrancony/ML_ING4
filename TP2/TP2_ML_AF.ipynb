{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Data management with Pandas\n",
    "Pandas is used to manipulate, clean, and query data by looking at the Pandass data tool kit. Pandass was created\n",
    "by Wes McKinny in 2008 and is an open source project under a very permissive license.\n",
    "\n",
    "## Exercise 1: Pandss Series data structure\n",
    "If you are familiar with Pandas and Series manipulation, you can skip this one and go to the second exercise. To\n",
    "start, you will create a new notebook using your local environment or colab environment. You have to name your\n",
    "notebook with PW2-[firstName Lastname MajorName]. Now you can answer the following questions\n",
    "1. import Pandas and create a list of sports containing this list [\n",
    "′Football′\n",
    ",\n",
    "′ HandBall′\n",
    ",\n",
    "′ SnowSport′\n",
    "] and\n",
    "store it in a variable sports;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a list of sports\n",
    "sports = ['Football', 'HandBall', 'SnowSport']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. cast your list as a series and display sports using the Class Pandass.Series (see documention on class Pandass.Series(data=None, index=None, dtype=None, name=None, copy=None, fastpath=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Football\n",
      "1     HandBall\n",
      "2    SnowSport\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "sports_series = pd.Series(sports)\n",
    "print(sports_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. what is the type of each element of your list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "for item in sports:\n",
    "    print(type(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create a new list named numeric containing a short list of numbers and display it as a series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     5\n",
      "1    10\n",
      "2    15\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "numeric = [5, 10, 15]\n",
    "numeric_series = pd.Series(numeric)\n",
    "print(numeric_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. add the value None to both animals and numeric and compare their corresponding display with Series.\n",
    "What did you remark?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      NaN\n",
      "1     15.0\n",
      "2    100.0\n",
      "dtype: float64\n",
      "0    BasketBall\n",
      "1      HandBall\n",
      "2          None\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "numeric_with_none = [None, 15, 100]\n",
    "sports_with_none = ['BasketBall', 'HandBall', None]\n",
    "\n",
    "numeric_series_with_none = pd.Series(numeric_with_none)\n",
    "sports_series_with_none = pd.Series(sports_with_none)\n",
    "\n",
    "print(numeric_series_with_none)\n",
    "print(sports_series_with_none)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "\n",
    "When None is added to the numeric list, it is converted to NaN in the series.\n",
    "In the sports list, None appears as an empty object in the series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. We shall correct the incomplete value None of sports by changing the None by NaN (Numpy.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    BasketBall\n",
      "1      HandBall\n",
      "2           NaN\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sports_with_nan = ['BasketBall', 'HandBall', np.nan]\n",
    "sports_series_with_nan = pd.Series(sports_with_nan)\n",
    "print(sports_series_with_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Let construct our variable sports from a dictionary as follows: {'BasketBall', 'HandBall', 'Snowsport',\n",
    "'baseBall', 'Swimming'}. Cast the variable to a serie and store it a new variable sIndex. Display sIndex\n",
    "and call the function index on it. What did you remark?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasketBall    HandBall\n",
      "Snowsport     baseBall\n",
      "Swimming          None\n",
      "dtype: object\n",
      "Index(['BasketBall', 'Snowsport', 'Swimming'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "sports_dict = {'BasketBall': 'HandBall', 'Snowsport': 'baseBall', 'Swimming': None}\n",
    "sIndex = pd.Series(sports_dict)\n",
    "\n",
    "print(sIndex)\n",
    "print(sIndex.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Series Querying\n",
    "Let we consider the variable sports from a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bask    BasketBall\n",
       "hand      HandBall\n",
       "snow     Snowsport\n",
       "base      baseBall\n",
       "swim      Swimming\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.isnan(np.nan)\n",
    "sports={'bask': 'BasketBall', 'hand': 'HandBall', 'snow': 'Snowsport', 'base': 'baseBall','swim': 'Swimming'}\n",
    "#cast the list on a serie\n",
    "sIndex=pd.Series(sports)\n",
    "#display the serie\n",
    "display(sIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. print the second element of the variable sports and find the element who has ’swim’. (use iloc[] to index\n",
    "location and loc[] to label location).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HandBall\n",
      "Swimming\n"
     ]
    }
   ],
   "source": [
    "# Print second element using iloc (index-based)\n",
    "print(sIndex.iloc[1])  # Second element based on position\n",
    "\n",
    "# Find the element labeled 'swim' using loc (label-based)\n",
    "print(sIndex.loc['swim'])  # Element with label 'swim'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Transform all items of your list sports in an uppercase characters (see Series.str.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bask    BASKETBALL\n",
      "hand      HANDBALL\n",
      "snow     SNOWSPORT\n",
      "base      BASEBALL\n",
      "swim      SWIMMING\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "sIndex_upper = sIndex.str.upper()\n",
    "print(sIndex_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. write a simple loop to transform sports in an uppercase Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    BASKETBALL\n",
      "1      HANDBALL\n",
      "2     SNOWSPORT\n",
      "3      BASEBALL\n",
      "4      SWIMMING\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "sIndex_upper_loop = pd.Series([item.upper() if item else None for item in sIndex])\n",
    "print(sIndex_upper_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. compare the runtime between both solutions and explain the runtime gap (see %timeit which is an IPython\n",
    "magic function, which can be used to time a particular piece of code.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.3 µs ± 6.18 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "69 µs ± 7.25 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Using Series.str.upper() with vectorization\n",
    "%timeit sIndex.str.upper()\n",
    "\n",
    "# Using a loop\n",
    "%timeit pd.Series([item.upper() if item else None for item in sIndex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "The Series.str.upper() function is vectorized and runs faster because Pandas internally optimizes operations on entire arrays.\n",
    "The loop, though simple, operates element by element, making it slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. compare the runtime between the function np.mean() and a loop to calculate the mean value of your numeric\n",
    "variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.2 µs ± 1.72 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "numeric = [None, 15, 100]\n",
    "num = pd.Series(numeric)\n",
    "%timeit np.mean(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.5\n"
     ]
    }
   ],
   "source": [
    "mean_value = 0\n",
    "total = 0\n",
    "count = 0\n",
    "for item in num:\n",
    "    if pd.notna(item):  # Skip NaN values\n",
    "        total += item\n",
    "        count += 1\n",
    "mean_value = total / count if count != 0 else 0\n",
    "print(mean_value)\n",
    "\n",
    "%timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of Runtime Difference:\n",
    "\n",
    "Vectorization (using np.mean()): This approach is faster since NumPy operations are highly optimized for performance and can handle entire arrays at once.\n",
    "Loop: It involves explicit iteration over elements, which is slower, especially for large datasets, because each operation is executed individually rather than leveraging efficient underlying array operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: data Verification\n",
    "## Exercise 3: From CSV to DataFrame\n",
    "As a first step, we need python libraries allowing us to load our data as a dataframe. You have to download the\n",
    "csv file named Custemers.csv. This file contains 15 columns separated with a ’,’ character.\n",
    "\n",
    "1. Load the file Custemers.csv and store its content into a dataframe variable Custemers Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file into a DataFrame (ensure the path to the file is correct)\n",
    "Custemers_Data = pd.read_csv('data/Custemers.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Display the shape of your dataframe and display the header to understand the meaning of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 15)\n",
      "                                                name  \\\n",
      "0                                       Deonte Stark   \n",
      "1                                     Faustino Boyer   \n",
      "2  Eddy Bogisich,33431 Dollie Squares Apt. 654,Po...   \n",
      "3                                     Mervyn Kreiger   \n",
      "4  Katlyn Doyle,4650 Beer Crossing Suite 848,Nort...   \n",
      "\n",
      "                        address             city           state         zip  \\\n",
      "0            278 Mueller Plains       North Euna         Alabama  03404-4384   \n",
      "1  70244 Skiles Falls Suite 030  North Altohaven      California  01522-1310   \n",
      "2                           NaN              NaN             NaN         NaN   \n",
      "3            376 Dorinda Stream     Shaniquafort  South Carolina  39347-4438   \n",
      "4                           NaN              NaN             NaN         NaN   \n",
      "\n",
      "                 phone                          email           work  \\\n",
      "0   (180)940-9676x4495           shanna73@hotmail.com     Hahn-Mayer   \n",
      "1  (308)699-6239x81011            ferrell81@gmail.com  Buckridge Inc   \n",
      "2                  NaN                            NaN            NaN   \n",
      "3         869-985-6299  emmerich.griselda@hotmail.com    Witting PLC   \n",
      "4                  NaN                            NaN            NaN   \n",
      "\n",
      "                     work address       work city work state work zipcode  \\\n",
      "0  8177 Weber Throughway Apt. 341        Jaronton      Maine   51589-1424   \n",
      "1              236 Kessler Center  New Gavynshire   Missouri   52234-6972   \n",
      "2                             NaN             NaN        NaN          NaN   \n",
      "3               521 Kemmer Manors        Nerytown   Kentucky        68774   \n",
      "4                             NaN             NaN        NaN          NaN   \n",
      "\n",
      "          work phone                 work email   account created on  \n",
      "0        01240240340  heller.kirstin@glover.com  2001-09-06 06:15:24  \n",
      "1  (486)896-6855x446   esta.dicki@bechtelar.com           1983-04-25  \n",
      "2                NaN                        NaN                  NaN  \n",
      "3      (212)169-8190        greyson39@purdy.com  1971-04-27 14:05:06  \n",
      "4                NaN                        NaN                  NaN  \n"
     ]
    }
   ],
   "source": [
    "# Display the shape of the DataFrame (rows, columns)\n",
    "print(Custemers_Data.shape)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(Custemers_Data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the dataframe.columns to display the dictionary indexing your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['name', 'address', 'city', 'state', 'zip', 'phone', 'email', 'work',\n",
      "       'work address', 'work city', 'work state', 'work zipcode', 'work phone',\n",
      "       'work email', 'account created on'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Display column names\n",
    "print(Custemers_Data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Catch missing values\n",
    "Now we want to evaluate the number of missing data in the hole data. Display the total of missing values in each\n",
    "column of custemers Data\n",
    "1. You can use dataframe.isnull() and verctorization to sum missing values by each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                    81\n",
      "address               3365\n",
      "city                  3374\n",
      "state                 3365\n",
      "zip                   3362\n",
      "phone                 3366\n",
      "email                 3363\n",
      "work                  3390\n",
      "work address          3383\n",
      "work city             3378\n",
      "work state            3390\n",
      "work zipcode          3378\n",
      "work phone            3377\n",
      "work email            3359\n",
      "account created on    3368\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in each column\n",
    "missing_values = Custemers_Data.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Display the rows with missing values in the ’name’ column, you should have 81 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     name                          address              city           state  \\\n",
      "148   NaN     558 Brycen Mission Suite 152        Cristmouth        Arkansas   \n",
      "273   NaN       0481 Sanford Lake Apt. 439     Bashirianberg  North Carolina   \n",
      "300   NaN    38689 Kimora Groves Suite 807    New Nadiahaven         Vermont   \n",
      "330   NaN       077 Walsh Summit Suite 123        Rogahnfurt         Indiana   \n",
      "467   NaN              87140 Loma Crescent   North Dixieport        Michigan   \n",
      "...   ...                              ...               ...             ...   \n",
      "9316  NaN               054 Aubrie Corners    East Genevieve   New Hampshire   \n",
      "9487  NaN       9957 Rempel Wells Apt. 081         New Micky         Alabama   \n",
      "9745  NaN  6277 Schneider Common Suite 939         Port Aura    North Dakota   \n",
      "9816  NaN               986 Brianne Shoals  Port Wilbertstad     Mississippi   \n",
      "9834  NaN             19233 Kreiger Meadow        Kleinville        Michigan   \n",
      "\n",
      "             zip               phone                            email  \\\n",
      "148   52585-7480    +03(6)2449148729               corene08@gmail.com   \n",
      "273   61948-9865  (967)713-7747x7329  hermiston.jenniffer@hotmail.com   \n",
      "300   99133-2546  595.341.7775x50639               sfritsch@yahoo.com   \n",
      "330        81369    +38(1)3931574807             tdenesik@hotmail.com   \n",
      "467   32196-8397    +65(0)2946604451              lucille69@gmail.com   \n",
      "...          ...                 ...                              ...   \n",
      "9316       92371    296-242-9406x586         enrico.kuvalis@yahoo.com   \n",
      "9487       24013        522-665-0735              rylan47@hotmail.com   \n",
      "9745       10127        127.858.7550                 kacy29@gmail.com   \n",
      "9816       99396    953.475.7297x373         ritchie.hadley@yahoo.com   \n",
      "9834  42537-0382   (894)710-7706x071            hjalmer14@hotmail.com   \n",
      "\n",
      "                   work                  work address         work city  \\\n",
      "148           Braun PLC    8795 Carissa Land Apt. 884  South Arachester   \n",
      "273           Grant LLC                7375 Lynn Fork      Wuckerthaven   \n",
      "300       Purdy-Farrell              29365 Nyah Flats     West Isabella   \n",
      "330        Corwin-Fahey  717 Karol Stravenue Apt. 964      Trantowhaven   \n",
      "467         Monahan Ltd         99220 Murphy Motorway     East Reyhaven   \n",
      "...                 ...                           ...               ...   \n",
      "9316      Schneider LLC          53056 Gaige Motorway    North Audriana   \n",
      "9487  Bergnaum and Sons   60120 Klocko Alley Apt. 615       Adalineport   \n",
      "9745         Becker PLC           186 Armstrong Lakes      East Rosalyn   \n",
      "9816       Kautzer-Batz  22243 Feil Terrace Suite 469      New Catofurt   \n",
      "9834        Beer-Ledner            48801 Bobbye Roads        West Tylor   \n",
      "\n",
      "        work state work zipcode          work phone  \\\n",
      "148     New Mexico   68070-4086         09359769734   \n",
      "273           Ohio   89342-4989        419-132-5389   \n",
      "300        Florida        06342    +66(4)7117753075   \n",
      "330           Utah   49873-6641   882.577.0021x2495   \n",
      "467   Rhode Island        50641        773.245.8906   \n",
      "...            ...          ...                 ...   \n",
      "9316      Arkansas        73816        605.101.0879   \n",
      "9487      Maryland   37182-8209   289.393.5802x9949   \n",
      "9745          Ohio        07576  1-924-436-1559x850   \n",
      "9816     Wisconsin        62178         09723145605   \n",
      "9834      Arkansas        60338    849.567.9201x682   \n",
      "\n",
      "                              work email   account created on  \n",
      "148   hezzie.hettinger@mcglynnlarson.com           1999-12-12  \n",
      "273                 colin.walsh@hane.com  1995-08-07 19:52:30  \n",
      "300                     ocrooks@mohr.net           1975-04-07  \n",
      "330          mschaden@gerholdschultz.net           03/10/2010  \n",
      "467            parisian.willard@feil.org           04/29/1986  \n",
      "...                                  ...                  ...  \n",
      "9316         cephus.conroy@schroeder.com           01/21/2000  \n",
      "9487                    mwolf@corwin.net           2000-09-07  \n",
      "9745         kunde.carry@auerweimann.org  1995-08-13 11:03:45  \n",
      "9816           thompson.eulalie@mann.com  1970-01-27 16:26:56  \n",
      "9834            hsatterfield@bradtke.com  2015-10-13 23:24:40  \n",
      "\n",
      "[81 rows x 15 columns]\n",
      "Number of rows with missing names: 81\n"
     ]
    }
   ],
   "source": [
    "# Display rows where 'name' column has missing values\n",
    "missing_names = Custemers_Data[Custemers_Data['name'].isnull()]\n",
    "print(missing_names)\n",
    "\n",
    "# Check the number of rows with missing names\n",
    "print(f\"Number of rows with missing names: {missing_names.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Try removing rows with missing names. We call dropna() function which has many arguments to scale the\n",
    "level of a filtered data:\n",
    "• axis=0 or 1 allows to filter the missing values according to the row(axis=0) or the column(axis=1)\n",
    "• how=all allows to eliminate all rows or all columns having mainly missing values\n",
    "• inplace = True allow to need the same dataframe without creating a new output\n",
    "• thresh= integer value allows to keep only rows (or columns) having a missing value rate more than\n",
    "thresh.\n",
    "test the function dropna() on your dataframe as:\n",
    "• dropna(how=’all’, inplace=True) and check if missing values are removed!\n",
    "• dropna(inplace=True, axis=0) and check if missing values are removed!\n",
    "What did you conclude?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                    81\n",
      "address               3365\n",
      "city                  3374\n",
      "state                 3365\n",
      "zip                   3362\n",
      "phone                 3366\n",
      "email                 3363\n",
      "work                  3390\n",
      "work address          3383\n",
      "work city             3378\n",
      "work state            3390\n",
      "work zipcode          3378\n",
      "work phone            3377\n",
      "work email            3359\n",
      "account created on    3368\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "Custemers_Data.dropna(how='all', inplace=True)\n",
    "# Check if any rows with all missing values were removed\n",
    "print(Custemers_Data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                  0\n",
      "address               0\n",
      "city                  0\n",
      "state                 0\n",
      "zip                   0\n",
      "phone                 0\n",
      "email                 0\n",
      "work                  0\n",
      "work address          0\n",
      "work city             0\n",
      "work state            0\n",
      "work zipcode          0\n",
      "work phone            0\n",
      "work email            0\n",
      "account created on    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "Custemers_Data.dropna(inplace=True, axis=0)\n",
    "# Check if rows with any missing values were removed\n",
    "print(Custemers_Data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use dropna(how='all'), it will only remove rows that are entirely made up of NaN values. If a row has even one non-NaN value, it will not be removed.\n",
    "When you use dropna(axis=0), it removes all rows that contain any missing value. If you had missing values scattered throughout the DataFrame, this would remove a significant number of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Ensure that values have the right format/type\n",
    "If we take the ”name” column, we find the first name and the last name separated with a blank character\n",
    "1. check the type of each column using the function dtype()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                  object\n",
      "address               object\n",
      "city                  object\n",
      "state                 object\n",
      "zip                   object\n",
      "phone                 object\n",
      "email                 object\n",
      "work                  object\n",
      "work address          object\n",
      "work city             object\n",
      "work state            object\n",
      "work zipcode          object\n",
      "work phone            object\n",
      "work email            object\n",
      "account created on    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check the data type of each column\n",
    "print(Custemers_Data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Let we split the column ’name’ into two columns where the first string is the first name and the second\n",
    "string is the last name. (see str.split()).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "StringMethods.split() takes from 1 to 2 positional arguments but 3 positional arguments (and 1 keyword-only argument) were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[164], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Split 'name' column into 'first_name' and 'last_name' columns\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m Custemers_Data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_name\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[43mCustemers_Data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(Custemers_Data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_name\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\strings\\accessor.py:136\u001b[0m, in \u001b[0;36mforbid_nonstring_types.<locals>._forbid_nonstring_types.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use .str.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with values of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferred dtype \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m     )\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: StringMethods.split() takes from 1 to 2 positional arguments but 3 positional arguments (and 1 keyword-only argument) were given"
     ]
    }
   ],
   "source": [
    "# Split 'name' column into 'first_name' and 'last_name' columns\n",
    "Custemers_Data[['first_name', 'last_name']] = Custemers_Data['name'].str.split(' ', 1, expand=True)\n",
    "print(Custemers_Data[['first_name', 'last_name']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. You can remark incorrect lastname values. Remove rows having lastname length up to 16 characters.\n",
    "Take 5 mn to outline the main ideas you’ve retained from these exercises (Part I and II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'last_name' has more than 16 characters\n",
    "Custemers_Data = Custemers_Data[Custemers_Data['last_name'].str.len() <= 16]\n",
    "print(Custemers_Data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the key ideas retained from these exercises:\n",
    "\n",
    "Data Management with Pandas: You learned how to import and load data from a CSV file into a Pandas DataFrame, how to check the structure of the DataFrame (using functions like shape(), head(), columns), and how to query data effectively.\n",
    "Handling Missing Data: You gained experience using Pandas functions like isnull(), dropna(), and filtering data based on missing values. We also explored different ways to handle missing data, like dropping rows or columns with missing values.\n",
    "String Operations: You practiced string manipulations such as splitting strings within a column into multiple columns and cleaning up data by handling incorrect values (e.g., removing rows with long last_name values).\n",
    "Vectorization vs Loops: You learned the benefits of vectorized operations, especially for performance, and how they compare to loops.\n",
    "Data Integrity: Ensuring that the data types and formats of columns are correct is essential for any further analysis or preprocessing steps in the machine learning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Data repairing with imputation\n",
    "## Exrcise 6: Do it from scratch at home and upload it on Moodle\n",
    "Now you have new datasets ”Olympics.csv” and ”flicker.csv” available on Moodle. You have to define your own\n",
    "strategy to clean these data and comment your notebook at each step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7: Imputation to handle missing data\n",
    "In this exercise we will compare two solutions of data cleaning. The first one consists to drop missing columns values while the second is to fill missing values with interesting ones. In your notebook, open the file melb data.csv.\n",
    "1. load your data as a dataframe and use a clear name as ”initialData”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "initialData = pd.read_csv('data/melb_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. observe first the quality of the data by using these functions: dataframe.columns, dataframe.info(), and\n",
    "dataframe.shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the column names\n",
    "print(initialData.columns)\n",
    "\n",
    "# Display info about data types and non-null counts\n",
    "print(initialData.info())\n",
    "\n",
    "# Display the shape (rows, columns) of the DataFrame\n",
    "print(initialData.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. display the total missing values by columns (axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total missing values by columns\n",
    "missing_by_columns = initialData.isnull().sum()\n",
    "print(missing_by_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. display total missing Values by rows (axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total missing values by rows\n",
    "missing_by_rows = initialData.isnull().sum(axis=1)\n",
    "print(missing_by_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. list the columns with missing values and store them in a variable colsWithMissing. Use the function isnull()\n",
    "and try to write a simple function you can call it with other datasets with the following signature and core\n",
    "code as following:\n",
    "If those columns had relevant information your model loses access to it when the column is dropped. Another drawback to this solution is to miss to do the same droping on the test dataset where an error will\n",
    "occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_columns(originDB):\n",
    "    # List of columns with missing values\n",
    "    colsWithMissing = [col for col in originDB.columns if originDB[col].isnull().any()]\n",
    "    \n",
    "    # Drop columns with missing values\n",
    "    reduced_original_data = originDB.drop(colsWithMissing, axis=1)\n",
    "    \n",
    "    return colsWithMissing, reduced_original_data\n",
    "\n",
    "# Call the function on initialData\n",
    "colsWithMissing, reduced_initialData = missing_columns(initialData)\n",
    "print(\"Columns with missing values:\", colsWithMissing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. display the rate of missing values by columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of missing values by columns\n",
    "missing_rate_columns = (initialData.isnull().sum() / len(initialData)) * 100\n",
    "print(missing_rate_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. do the same on the rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of missing values by rows\n",
    "missing_rate_rows = (initialData.isnull().sum(axis=1) / initialData.shape[1]) * 100\n",
    "print(missing_rate_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. remove rows whom the rate of missing values are > 5% from the origin data and store the result on a new\n",
    "dataframe variable named new Data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for removing rows with more than 5% missing values\n",
    "threshold = 0.05 * initialData.shape[1]\n",
    "\n",
    "# Remove rows with missing value rate > 5%\n",
    "newData = initialData[initialData.isnull().sum(axis=1) <= threshold]\n",
    "print(\"Shape of newData:\", newData.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. call your function missing colums(originDB) on both original data and on new data obtained after removal\n",
    "rows. How do you explain the columns difference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call on initialData\n",
    "colsWithMissing_initial, reduced_initialData = missing_columns(initialData)\n",
    "\n",
    "# Call on newData after removing rows\n",
    "colsWithMissing_new, reduced_newData = missing_columns(newData)\n",
    "\n",
    "print(\"Columns with missing values in initial data:\", colsWithMissing_initial)\n",
    "print(\"Columns with missing values in new data:\", colsWithMissing_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.  fill the missing values with the mean price, so you have to: 1) Display the statistical description of the\n",
    "column Price using the function describe(), 2) then to calculate the mean value and 3) to fill the missing\n",
    "value with the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical description of the 'Price' column\n",
    "print(newData['Price'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the 'Price' column\n",
    "price_mean = newData['Price'].mean()\n",
    "print(\"Mean Price:\", price_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in 'Price' with the mean value\n",
    "newData['Price'].fillna(price_mean, inplace=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
